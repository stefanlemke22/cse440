{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvxZfmwuv7Fe"
      },
      "source": [
        "# Lab 2: Frozen Lake\n",
        "In this semester's lab on reinforcement learning, we will be using the [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment. The Frozen Lake environment is a classic reinforcement learning problem commonly used for teaching and experimenting with RL algorithms. It is part of the [OpenAI Gym library](https://gymnasium.farama.org/) and is modeled after the grid world problem.\n",
        "\n",
        "In this environment, we need to start from a fixed point and bypass the ice holes we may encounter in order to reach the fixed end point and get the gift. At each position, we have four actions that can be performed next: walk up, walk down, walk left and walk right. If we fall into an ice hole, then the *current* round is over; on the other hand, if we reach the end, we will get one reward point.\n",
        "\n",
        "**Submission Requirement:**\n",
        "1. Finish all the TODO parts.\n",
        "2. A written report including your understanding, comments, descriptions, thoughts ... for all the code snippets in this notebook should be also submitted to D2L in digital format (e.g., .pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUsHI8k2C7a"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RGDRPK28PI7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# You will need numpy and gym. You can try running the following lines to install them\n",
        "# The assignment is tested on Python3.8 so in case you are having installation issues you might\n",
        "# want to try installing that version.\n",
        "\n",
        "!{os.sys.executable} -m pip install numpy\n",
        "!{os.sys.executable} -m pip install gymnasium\n",
        "!{os.sys.executable} -m pip install Pillow\n",
        "!{os.sys.executable} -m pip install ipython\n",
        "!{os.sys.executable} -m pip install pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdZYuiwi376T"
      },
      "source": [
        "## Part 1: Introduction to Frozen Lake\n",
        "This part will introduce the basic setup and control methods of Frozen Lake, including two tasks to help you get started quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjtBznjE9PKx"
      },
      "outputs": [],
      "source": [
        "# Import the environment we will use in this assignment\n",
        "env=gym.make('FrozenLake-v1',is_slippery=False, render_mode=\"rgb_array\").unwrapped\n",
        "\n",
        "# Show the model\n",
        "print(f\"Number of States {env.observation_space.n}, Number of Actions {env.action_space.n}\")\n",
        "print(f\"Reward range {env.reward_range}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPs5SLGH9VAW"
      },
      "outputs": [],
      "source": [
        "# reset the environment\n",
        "env.reset()\n",
        "# visualize the current state\n",
        "display(Image.fromarray(env.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T93AIZeQ46Iu"
      },
      "source": [
        "Here is the mapping between actions and indices:\n",
        "```\n",
        "0: Move left\n",
        "1: Move down\n",
        "2: Move right\n",
        "3: Move up\n",
        "```\n",
        "The following are two examples to help you get familiar with the environment. There is no task you need to finish in examples.\n",
        "### Example 1\n",
        "This is an example showing how to control the player to walk in a given direction. Be sure to reset the environment before starting a new round to ensure starting from the initial position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vCGU6lh9wGL"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "action_1 = 1 # move down\n",
        "state, reward, finished, _, _ = env.step(action_1) # input the action instruction to env\n",
        "print(\"state index=\", state, \"reward=\", reward, \"is_finished=\", finished)\n",
        "\n",
        "action_2 = 1 # move down again\n",
        "state, reward, finished, _, _ = env.step(action_2) # input the action instruction to env\n",
        "print(\"state index=\", state, \"reward=\", reward, \"is_finished=\", finished)\n",
        "\n",
        "# display the environment state\n",
        "display(Image.fromarray(env.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhtDBdEyC3pb"
      },
      "source": [
        "This is another example showing what happens if the player falls into an ice hole. In this example, we'll take advantage of Python's ability to use foor loops to perform actions sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQK6OfvE9wVY"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "actions = [\n",
        "    2, # move right\n",
        "    2, # move right\n",
        "    2, # move right\n",
        "    1, # move down\n",
        "]\n",
        "for action in actions:\n",
        "  state, reward, finished, _, _ = env.step(action) # input the action instruction to env\n",
        "  print(\"state index=\", state, \"reward=\", reward, \"is_finished=\", finished)\n",
        "# display the env state\n",
        "display(Image.fromarray(env.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7KrEV9Kp4pf"
      },
      "source": [
        "### Task 1: Navigate to the end\n",
        "Now it's your turn to avoid all the ice holes and get the gift. You only need to give one path to reach the end. Don't forget to **reset the environment before everything starts**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99eR1ULuCXvr"
      },
      "outputs": [],
      "source": [
        "#################TODO#####################\n",
        "\n",
        "#############################################\n",
        "display(Image.fromarray(env.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al8mEshWqUfE"
      },
      "source": [
        "### Task 2: Sliding on the ice\n",
        "In a more complex setting, we introduce the concept of slippery. This means that there is only a 1/3 probability that you will move in your specified direction, and a 1/3 probability that you will slide vertically up or down, respectively. For example, if you want to walk to the right, then\n",
        "```\n",
        "P(move right) = 1/3\n",
        "P(move up) = 1/3\n",
        "P(move down) = 1/3\n",
        "```\n",
        "In this task, you need to re-execute the actions in Task 1 to see if you can still end up with the gift. You **don't have to get the gift**, but we expect you to be aware of the effect of randomness on the policy (i.e., action plan).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YZnmVgspsb6"
      },
      "outputs": [],
      "source": [
        "# load the environmet in new setting\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True, render_mode=\"rgb_array\").unwrapped\n",
        "#################TODO#####################\n",
        "\n",
        "#############################################\n",
        "display(Image.fromarray(env.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s_wIa2nC3pe"
      },
      "source": [
        "## Part 2: MDP and Policy Iteration\n",
        "From now on, we will focus on considering the sliding environment setting, as it is more in line with what we study in reinforcement learning.\n",
        "### Environment Model\n",
        "The environment model is a command for retrieving the current state of the environment, which can help us in constructing the MDP algorithm. This command contains two parameters State and Action:\n",
        "1. State refers to the index of a specific position in the environment. For example, for a fixed 4*4 map, the top row has positions indexed 0, 1, 2, 3, where the index of the top-left start position is 0, and the bottom row has positions 12, 13, 14, 15, where the index of the end position is 15.\n",
        "2. Action here refers to the index 0-3 of the movement we mentioned in the previous part.\n",
        "\n",
        "Given the current position STATE and the next action ACTION, we can obtain the environment model very easily via\n",
        "```\n",
        "env.P[STATE][ACTION]\n",
        "```\n",
        "The return value includes a list of tuples, where each tuple contains four values: probability, the new position after moving, the reward, and whether the episode has ended. Following is an example, you may need to run this ceil several times to see what happens when you slip and don't slip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLAUhaTuC3pf"
      },
      "outputs": [],
      "source": [
        "# try to run several times to see the difference between slippery and non-slippery\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True, render_mode=\"rgb_array\").unwrapped\n",
        "env.reset()\n",
        "random_state  = env.observation_space.sample()\n",
        "random_action = env.action_space.sample()\n",
        "print(\"Position index=\", random_state)\n",
        "print(\"Action index=\", random_action)\n",
        "# returns a list of tuples (probability,newstate,reward,is_terminal_state)\n",
        "env.P[random_state][random_action]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMgeIjUkC3pf"
      },
      "source": [
        "### Task 1: Implement Policy Evaluation\n",
        "In this task you need to complete the `policy_evaluation` function, which iteratively update the value of each state according to the policy until the value function converges.\n",
        "<div>\n",
        "    <img src=\"https://github.com/Michigan-State-University-CSE-440/Frozen-Lake-and-MDP/blob/main/assets/policy_eva.png?raw=1\" alt=\"Description\" width=\"800\" height=\"150\">\n",
        "</div>\n",
        "\n",
        "#### Step-by-Step Implementation Instructions\n",
        "\n",
        "1. **For each iteration**:\n",
        "   - Create a new array to store updated values for all states (e.g., `new_value_function`).\n",
        "   - For each state `s`, look up the policy’s action `a`. Use the environment model `P[s][a]` to:\n",
        "     - Calculate the expected return by summing over all transitions `(prob, next_state, reward, done)`.\n",
        "     - Write this computed value into `new_value_function[s]`.\n",
        "   - Compare `new_value_function` to your current `value_function` and record the largest change (`delta`).\n",
        "   - Assign `value_function = new_value_function` for the next iteration.\n",
        "\n",
        "2. **Stop** when the largest change (`delta`) falls below the specified tolerance (`tol`).\n",
        "\n",
        "By doing this, the function will evaluate how good the policy is (i.e., compute a stable estimate of the return from each state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gsSKSQPC3pf"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(P, nS, policy, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Evaluate the value function from a given policy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    P : Environment model where:\n",
        "        P[0]: Probability of transitioning from state s to next_state using action a.\n",
        "        P[1]: The next state after taking action a.\n",
        "        P[2]: The immediate reward for this transition.\n",
        "        P[3]: A boolean that indicates if the next_state is terminal.\n",
        "    nS : int\n",
        "        The number of states in the environment.\n",
        "    policy : np.ndarray of shape [nS]\n",
        "        The policy to evaluate. Maps each state s to the action a that should be taken.\n",
        "    gamma : float, optional (default=0.9)\n",
        "        The discount factor, must be in the range [0,1).\n",
        "        Higher gamma discounts future rewards more slowly.\n",
        "    tol : float, optional (default=1e-3)\n",
        "        Tolerance for convergence of the value function. The evaluation stops when\n",
        "        max |new_value_function[s] - value_function[s]| < tol.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    value_function : np.ndarray of shape [nS]\n",
        "        The value function for the given policy. value_function[s] gives the expected\n",
        "        return (discounted sum of future rewards) when starting from state s and\n",
        "        following the given policy forever.\n",
        "    \"\"\"\n",
        "    # Initialize the value function with zeros\n",
        "    value_function = np.zeros(nS)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        ######################TODO#######################\n",
        "\n",
        "        #################################################\n",
        "\n",
        "    return value_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giT51uxHC3pf"
      },
      "source": [
        "Test your policy_evaluation on 5 randomly generated deterministic policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d21GepQiC3pg"
      },
      "outputs": [],
      "source": [
        "# evaluate random policy\n",
        "n_state = env.observation_space.n\n",
        "for i in range(5):\n",
        "   random_policy = np.random.randint(4, size=n_state)\n",
        "   print(f'-------- Policy {i}','-'*30)\n",
        "   print(policy_evaluation(P=env.P, nS=n_state, policy=random_policy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3eUhAcbC3pg"
      },
      "source": [
        "### Task 2: Implement Policy Improvement\n",
        "In this task, you need to complete the `policy_improvement` function so that it uses **one-step look-ahead** to compute a better policy, given a value function `value_from_policy`.\n",
        "\n",
        "Below is the equation we learned in class (represented as an image):\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/Michigan-State-University-CSE-440/Frozen-Lake-and-MDP/blob/main/assets/policy_imp.png?raw=1\" alt=\"Description\" width=\"800\" height=\"150\">\n",
        "</div>\n",
        "\n",
        "This can be broken down into the following steps:\n",
        "\n",
        "1. **Compute `Q(s, a)` for each action**.  \n",
        "2. **Find the action** \\(a\\) **that maximizes** `Q(s, a)` and **set** `new_policy[s]` to that action.\n",
        "\n",
        "#### Step‐by‐Step Implementation Instructions\n",
        "\n",
        "1. **Loop over all states** `s` in `range(nS)`.\n",
        "2. **For each state** `s`, **loop over all actions** `a` in `range(nA)`.\n",
        "3. **Compute** `Q(s, a)` by summing:  \n",
        "   ```\n",
        "   prob * (reward + gamma * value_from_policy[next_state])\n",
        "   for each `(prob, next_state, reward, done)` in `P[s][a]`.\n",
        "   ```\n",
        "4. **Keep track** of which `a` produces the largest `Q(s, a)` value and store this action in `new_policy[s]`.\n",
        "5. **Return** `new_policy` at the end.\n",
        "\n",
        "**Hint:** Initialize a variable like `best_q = float('-inf')` (negative infinity) before checking which action gives the highest `Q(s, a)`. This ensures you can handle negative values if needed.\n",
        "\n",
        "When you finish, your `policy_improvement` function will pick the **best action** in each state, given the value function `value`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7PURyMDC3pg"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(P, nS, nA, value, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Given the value function V(s) from a policy π, improve the policy by taking a one-step look-ahead\n",
        "    and choosing the action that maximizes expected return.\n",
        "\n",
        "    The one-step look-ahead uses:\n",
        "        π_{new}(s) = arg max_a  ∑_{s'}  P(s'|s,a) [ R(s,a,s') + γ * V(s') ]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    P : Environment model where:\n",
        "        P[0]: Probability of transitioning from state s to next_state using action a.\n",
        "        P[1]: The next state after taking action a.\n",
        "        P[2]: The immediate reward for this transition.\n",
        "        P[3]: A boolean that indicates if the next_state is terminal.\n",
        "    nS : int\n",
        "        Number of states in the environment.\n",
        "    nA : int\n",
        "        Number of actions in the environment.\n",
        "    value : np.ndarray of shape [nS]\n",
        "        The value function V(s) computed from the current policy π.\n",
        "    gamma : float, optional (default=0.9)\n",
        "        Discount factor, must be in [0,1). High γ puts more weight on future rewards.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    new_policy : np.ndarray of shape [nS]\n",
        "        The *improved* policy, where new_policy[s] is the action that maximizes the\n",
        "        one-step look-ahead given value_from_policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the new policy array\n",
        "    new_policy = np.zeros(nS, dtype=int)\n",
        "    ###################TODO#####################\n",
        "\n",
        "    ############################################\n",
        "    return new_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi7qiIdRC3pg"
      },
      "source": [
        "Print the value before and after policy improvements for 5 randomly generated policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkYfGAyQC3ph"
      },
      "outputs": [],
      "source": [
        "n_state = env.observation_space.n\n",
        "for i in range(5):\n",
        "   random_policy = np.random.randint(4, size=n_state)\n",
        "   print(f'-------- Policy {i}','-'*30)\n",
        "   pre_value_func = policy_evaluation(env.P, n_state, random_policy)\n",
        "   print(pre_value_func) # Print value function before improvement\n",
        "\n",
        "   print(f'-------> Policy {i} IMPROVED','-'*21)\n",
        "   improved_policy = policy_improvement(env.P, n_state, env.action_space.n, pre_value_func)\n",
        "   imp_value_func = policy_evaluation(env.P, n_state, improved_policy)\n",
        "   print(imp_value_func) # Print value after improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RZtd8T0C3ph"
      },
      "source": [
        "### Policy Iteration\n",
        "This `policy_iteration` function calls the `policy_evaluation` and `policy_improvement` functions to implement the full policy iteration algorithm. **There is no TODO part in this function.** You only need to run it after understanding how it works.\n",
        "\n",
        "#### Function Interpretation\n",
        "1. **Policy Evaluation**:\n",
        "   - Call `policy_evaluation(P, nS, nA, policy, gamma, tol)` to compute the value function under the **current** policy.\n",
        "\n",
        "2. **Policy Improvement**:\n",
        "   - Call `policy_improvement(P, nS, nA, value_function, policy, gamma)` to get a **new** policy that improves upon the current one.\n",
        "\n",
        "3. **Check for Policy Stability**:\n",
        "   - Compare the new policy with the old policy. If they are the **same** for all states, the policy is stable and you can **break** out of the loop.\n",
        "   - Otherwise, set the current policy to the new policy and **continue** iterating.\n",
        "\n",
        "4. **Result**:\n",
        "   - By the end of this process, you should have a **stable** policy that *cannot* be improved further with respect to the environment model `P`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpgZtqzyC3ph"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Run policy iteration for dynamics of P.\n",
        "\n",
        "    You should use your methods: policy_evaluation() and policy_improvement() here\n",
        "\n",
        "    Parameters:\n",
        "    P, nS, nA, gamma: defined at beginning of file\n",
        "    tolerance:        tolerance parameter used in policy_evaluation()\n",
        "\n",
        "    Returns:\n",
        "    value_function: np.ndarray[nS]\n",
        "    policy:         np.ndarray[nS]\n",
        "    \"\"\"\n",
        "    value_function = np.zeros(nS)\n",
        "    policy = np.zeros(nS, dtype=int)\n",
        "    while(True):\n",
        "      value_function = policy_evaluation(P, nS, policy, gamma, tol)\n",
        "      new_policy = policy_improvement(P, nS, nA, value_function, gamma)\n",
        "\n",
        "      policy_stable = True\n",
        "      for i in range(len(policy)):\n",
        "        if new_policy[i] != policy[i]:\n",
        "          policy_stable = False\n",
        "          break\n",
        "      if policy_stable is True:\n",
        "        break\n",
        "      else:\n",
        "        policy = new_policy\n",
        "\n",
        "    return value_function, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3EO5IegC3ph"
      },
      "source": [
        "### Task 3: Call Policy Iteration and Do Evaluation\n",
        "In this task, first you need to set the gamma value and call the `policy_iteration` function to get the optimal policy. Then you need to call the `follow_policy` function to test your policy and see the results.\n",
        "\n",
        "**Hint:**  \n",
        "- You need to **try different gamma values** to finsh the following task 4.\n",
        "- Try again if you cannot reach the end point using the strategy generated by the `policy_iteration` function, which is due to the randomness of state transitions. Usually it only takes two or three tries to succeed. If it never works, you probably did not complete the `policy_evaluation` or (and) `policy_improvement` functions correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utQUYXMIC3ph"
      },
      "outputs": [],
      "source": [
        "tolerance = 0.001\n",
        "##############################TODO##############################\n",
        "gamma =\n",
        "value, policy =\n",
        "################################################################\n",
        "print(\"Value Function:\", value)\n",
        "print(\"Policy:\", policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N8il9ivC3ph"
      },
      "outputs": [],
      "source": [
        "def follow_policy(policy):\n",
        "    \"\"\"\n",
        "    Follows the given policy from start (state 0) to goal (state 15).\n",
        "\n",
        "    Parameters:\n",
        "    policy (list): The policy array where each index represents the state and\n",
        "                   the value represents the action (0=Left, 1=Down, 2=Right, 3=Up).\n",
        "\n",
        "    Returns:\n",
        "    actions (list): The sequence of actions taken.\n",
        "    env (object): The environment object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Start at state 0 (top-left corner)\n",
        "    state = 0\n",
        "    done = False\n",
        "    actions_taken = []  # Track actions\n",
        "    env.reset()\n",
        "    while done == False:\n",
        "        action = policy[state]  # Get action from policy\n",
        "        actions_taken.append(action)\n",
        "        # Take action and observe new state\n",
        "        state, _, done, _, _ = env.step(action)\n",
        "\n",
        "    return actions_taken, env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tJDlwAJC3ph"
      },
      "outputs": [],
      "source": [
        "actions, states = follow_policy(policy)\n",
        "print(\"Actions Taken:\", actions)\n",
        "display(Image.fromarray(states.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3eIrszzC3ph"
      },
      "source": [
        "### Task 4 (Written Question):\n",
        "**Question**: After experimenting with various `gamma` values, what impact did you observe on the policy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8o2Mm_xC3pi"
      },
      "source": [
        "##############################TODO##############################  \n",
        "\n",
        "**Answer:**\n",
        "\n",
        "################################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}