{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjnkYpKDBcrH"
      },
      "source": [
        "# Lab 4 Logistic Regression\n",
        "\n",
        "Welcome to the world of Machine Learning!\n",
        "\n",
        "In this lab course, we will build a multi-label classifier based on logistic regression and apply it to a small-scale but very popular dataset.\n",
        "\n",
        "**Submission Requirement:**\n",
        "1. Submit .ipynb notebook file including all the running results. Failed to do so will cause grade deduction.\n",
        "2. Submit a lab report as a digital file (.pdf format is preferred) containing your descriptions, comments, and thoughts on code snippets that you think are important or of interest (not only the implemented tasks). This time I will provide more hints on potentially important parts, which are for reference only and are not necessary to be included in the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCVEQKFsBcrI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VekUF8NBcrJ"
      },
      "outputs": [],
      "source": [
        "## download dataset\n",
        "if not os.path.exists('Celebrity_Faces_Dataset'):\n",
        "    remote_path = 'https://github.com/Michigan-State-University-CSE-440/logistic-regression/releases/download/v1.0/Celebrity_Faces_Dataset.zip'\n",
        "    local_path = 'Celebrity_Faces_Dataset.zip'\n",
        "    if not os.path.exists(local_path):\n",
        "        os.system(f'wget {remote_path}')\n",
        "    os.system(f'unzip {local_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHXMuBFBcrK"
      },
      "source": [
        "## Part 1: Celebrity Faces Dataset\n",
        "This dataset consists of facial images of 17 different celebrities, each includes 100 images. The images are typically sourced from public datasets or collected from online media, ensuring a diverse range of facial expressions, lighting conditions, and angles.\n",
        "\n",
        "**Key Features:**\n",
        "- Total Celebrities: 17\n",
        "- Images per Celebrity: 100\n",
        "- Total Images: 1,700\n",
        "- Variation: Contains different facial poses, lighting conditions, and minor occlusions.\n",
        "- Annotation: Each image is annotated with one label (name of a celebrity) which can be accessed by the name of the corrosponding folder.\n",
        "\n",
        "**Dataset Folder Organization:**\n",
        "```\n",
        "Celebrity_Faces_Dataset\n",
        "  -Celebrity Name_1\n",
        "    -Image_1_Name.Png\n",
        "    -Image_2_Name.Png\n",
        "    -...\n",
        "  -Celebrity Name_2\n",
        "    -Image_1_Name.Png\n",
        "    -Image_2_Name.Png\n",
        "    -...\n",
        "  -...\n",
        "```\n",
        "\n",
        "Following is the visualizaiton of some images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHbN7bE3BcrK"
      },
      "outputs": [],
      "source": [
        "root_dir = 'Celebrity_Faces_Dataset'\n",
        "celebrity_names = sorted(os.listdir(root_dir))  # Ensure consistent ordering\n",
        "\n",
        "rows, cols = 2, 9\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(15, 6))  # Adjust figure size for better spacing\n",
        "# Iterate over celebrity folders\n",
        "for i, name in enumerate(celebrity_names):\n",
        "    if i >= rows * cols:  # Prevent out-of-bounds errors\n",
        "        break\n",
        "    # Read the first image of each celebrity\n",
        "    img_path = os.path.join(root_dir, name, os.listdir(os.path.join(root_dir, name))[0])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    row, col = divmod(i, cols)\n",
        "    axs[row, col].imshow(img)\n",
        "    axs[row, col].set_title(name, fontsize=10)\n",
        "    axs[row, col].axis('off')\n",
        "\n",
        "# Hide empty subplots\n",
        "for i in range(rows * cols):\n",
        "    if i >= len(celebrity_names):\n",
        "        row, col = divmod(i, cols)\n",
        "        axs[row, col].axis('off')\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(wspace=1.5, hspace=-0.5)  # Increased horizontal and vertical spacing\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qvDNVB2BcrL"
      },
      "source": [
        "### Task 1: Load Image Data\n",
        "In this task, you need to load all the images and assign them a corrosponding label. You may refer to the above visualization code to use OPENCV (cv2) python package for reading the images, but for this task you also need to do some pro-processings to the raw images, ensuring they can be smoothly used as the input of model.\n",
        "\n",
        "Learning Resources about the Usage of OPENCV:\n",
        "1. (official doc) https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html\n",
        "2. https://www.geeksforgeeks.org/opencv-python-tutorial/\n",
        "3. https://www.youtube.com/watch?v=kSqxn6zGE0c\n",
        "4. https://www.youtube.com/watch?v=oXlwWbU8l2o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKZcBbImBcrL"
      },
      "outputs": [],
      "source": [
        "def load_celebrity_faces(root_dir='Celebrity_Faces_Dataset', image_size=(64, 64)):\n",
        "    \"\"\"\n",
        "    Loads images from subfolders of `root_dir` (one folder per celebrity).\n",
        "    Returns:\n",
        "        X_gray: Flattened grayscale images for training, shape (num_samples, H*W).\n",
        "        y:      Label array, shape (num_samples,).\n",
        "        label_map: {label_idx: 'celebrity_name'}.\n",
        "        X_color:  Color images (resized but not converted to grayscale),\n",
        "                  shape (num_samples, H, W, 3) in RGB order for display.\n",
        "    \"\"\"\n",
        "    X_gray = []\n",
        "    X_color = []\n",
        "    y = []\n",
        "\n",
        "    # subfolders = celebrity names\n",
        "    classes = sorted([\n",
        "        d for d in os.listdir(root_dir)\n",
        "        if os.path.isdir(os.path.join(root_dir, d))\n",
        "    ])\n",
        "\n",
        "    ########################### Potentially Important: Name to Label Mapping ###########################\n",
        "    ## In machine learning algorithms, text cannot be used directly as a label because it cannot be converted to an one-hot encoding\n",
        "    ## (e.g., 1-> 010000, 5-> 000001). Therefore, we map each unique text to a unique numerical value (e.g., \"Angelina Jolie\"->0, \"Brad Pitt\"->1)\n",
        "    ## and use that numerical value as the label.\n",
        "    label_map = {idx: celeb for idx, celeb in enumerate(classes)}\n",
        "    name_to_label = {celeb: idx for idx, celeb in enumerate(classes)}\n",
        "    ###################################################################################################\n",
        "\n",
        "    for celeb_name in classes:\n",
        "        celeb_label = name_to_label[celeb_name]\n",
        "        celeb_folder = os.path.join(root_dir, celeb_name)\n",
        "        for i, filename in enumerate(os.listdir(celeb_folder)):\n",
        "            filepath = os.path.join(celeb_folder, filename)\n",
        "\n",
        "            ######TODO:Read color image img_bgr from filepath######\n",
        "            img_bgr =\n",
        "            #######################################################\n",
        "\n",
        "            ######TODO:Process color image img_bgr for result visualization######\n",
        "            # step 1: resize the image to 256*256\n",
        "            img_bgr_vis =\n",
        "            ######################################################################\n",
        "            # step 2: switch the order of color channel from BGR to RGB\n",
        "            img_rgb_vis = cv2.cvtColor(img_bgr_vis, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            ######TODO:Process color image img_bgr for model training######\n",
        "            # step 1: resize the image based on the input image_size\n",
        "            img_bgr =\n",
        "            # step 2: convert the image to grayscale, and flatten it to a 1D array\n",
        "            img_gray =\n",
        "            ##########################################################################\n",
        "\n",
        "            X_color.append(img_rgb_vis)\n",
        "            X_gray.append(img_gray)\n",
        "            y.append(celeb_label)\n",
        "\n",
        "    X_color = np.array(X_color, dtype=np.uint8)  # shape: (num_samples, H, W, 3)\n",
        "    X_gray = np.array(X_gray, dtype=np.float32)  # shape: (num_samples, H*W)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    return X_gray, y, label_map, X_color\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQiEx_iDBcrM"
      },
      "source": [
        "Validate the data loading function. The expected results should be 1700 samples in 17 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzFacZWvBcrM"
      },
      "outputs": [],
      "source": [
        "print('Data Statistics:')\n",
        "X_gray, y, label_map, X_color = load_celebrity_faces()\n",
        "print(\"Number of samples:\", len(X_gray))\n",
        "print(\"Number of classes:\", len(label_map))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcV7VgTGBcrN"
      },
      "source": [
        "### Task 2: K-folds Cross-validation\n",
        "K-fold cross-validation is a very classical technique used to evaluate machine learning models by splitting the dataset into K folds. In the case of multi-class classification, this ensures that images from each class are evenly distributed across the folds.\n",
        "\n",
        "**How It Works:**\n",
        "1. Partition the Dataset: Each class's images are divided into K equal parts (folds).\n",
        "2. Iterate K Times: For each iteration, one fold from each class is selected as the test sample for this category, and the test samples from all the classes are combined as the test data. Similarly, the remaining K-1 folds of each class are combined and used as the training data.\n",
        "3. The model should be **initalized and trained independently on each iteration**, thus we can calculate the classification accuracy for each iteration. The final accuracy is the average of all the accuracies over K iterations.\n",
        "\n",
        "In this task, you need to complete some key steps of K-folds data splitting so that the resulting data can be used for subsequent model training as well as cross-validation. Following is a detailed **Guideline for TODO part**:\n",
        "1. Initialize label_folds: A dictionary to store K folds for each class. (Already provided, no need to repeat this)\n",
        "2. Loop Through Each Class:\n",
        "   - Get the list of sample indices for the class using *label_indices_map*.\n",
        "   - Compute fold_sizes, ensuring that each fold gets an even share.\n",
        "3. Create Folds:\n",
        "   - Use a loop to slice the indices based on fold_sizes.\n",
        "   - Store each subset in label_folds.\n",
        "4. The resulting *label_folds* should be a dictionary where each *key* is a label, the corrosponding *value* is a list of folds, and each fold is a numpy array, i.e.,\n",
        "``label_folds = {0: [array_1, ..., array_k], 1: [array_1, ..., array_k], ..., K: [array_1, ..., array_k]}``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArVDdt6yBcrN"
      },
      "outputs": [],
      "source": [
        "def kfold_split(X, y, k=5, classes=17):\n",
        "    \"\"\"\n",
        "    Splits the dataset into k folds for cross-validation.\n",
        "    Args:\n",
        "        X: Input features, shape (num_samples, num_features).\n",
        "        y: Labels, shape (num_samples,).\n",
        "        k: Number of folds.\n",
        "    Returns:\n",
        "        folds: List of k tuples, each containing (train_indices, test_indices).\n",
        "    \"\"\"\n",
        "    assert len(X) == len(y), \"X and y must have same length.\"\n",
        "\n",
        "    label_indices_map = {}\n",
        "\n",
        "    ########################### Potentially Important: Gather Indices by Label ###########################\n",
        "    ## you may check the usage of np.where() function and figure out what's going on here, and think about\n",
        "    ## how it helps split the data of each class into k folds.\n",
        "    for label in range(classes):\n",
        "        label_indices = np.where(y == label)[0]\n",
        "        label_indices_map[label] = label_indices\n",
        "    ####################################################################################################\n",
        "\n",
        "    label_folds = {}\n",
        "    #######TODO: partition each label's indices into k folds######\n",
        "\n",
        "    ##############################################################\n",
        "\n",
        "    ######Potentially Important: combine folds across labels #######\n",
        "    ## Figure out how the indices are combined across labels to\n",
        "    ## form the final k folds.\n",
        "    folds = []\n",
        "    for i in range(k):\n",
        "        test_indices_list = []\n",
        "        train_indices_list = []\n",
        "        for label in range(classes):\n",
        "            # i-th subset for label -> test\n",
        "            test_indices_list.append(label_folds[label][i])\n",
        "            # other subsets -> train\n",
        "            for j in range(k):\n",
        "                if j != i:\n",
        "                    train_indices_list.append(label_folds[label][j])\n",
        "\n",
        "        test_indices = np.concatenate(test_indices_list)\n",
        "        train_indices = np.concatenate(train_indices_list)\n",
        "        folds.append((train_indices, test_indices))\n",
        "    ##############################################################\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1znR9zYBcrO"
      },
      "source": [
        "Now we are going to use the following code to verify our implementation of the K-folds data splitting. The expected output figure is shown below for reference:\n",
        "![image](https://github.com/Michigan-State-University-CSE-440/logistic-regression/blob/main/assets/k_fold_vis.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfWtoxDjBcrO"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Create a dummy dataset.\n",
        "# ------------------------------\n",
        "X_dummy = np.arange(1700).reshape(1700, 1)  # Dummy features.\n",
        "y_dummy = np.repeat(np.arange(17), 100)  # Dummy labels.\n",
        "print(\"Shape of X_dummy:\", X_dummy.shape)\n",
        "print(\"Shape of y_dummy:\", y_dummy.shape)\n",
        "\n",
        "# Generate the folds.\n",
        "folds = kfold_split(X_dummy, y_dummy, k=5)\n",
        "\n",
        "# ------------------------------\n",
        "# Visualization of Data Distribution.\n",
        "# ------------------------------\n",
        "unique_labels = np.unique(y_dummy)\n",
        "overall_counts = np.bincount(y_dummy, minlength=len(unique_labels))\n",
        "\n",
        "# Create a figure with one extra subplot for overall distribution.\n",
        "fig, axes = plt.subplots(1, len(folds) + 1, figsize=(18, 3))\n",
        "\n",
        "# Overall distribution plot.\n",
        "axes[0].bar(unique_labels, overall_counts, tick_label=unique_labels)\n",
        "axes[0].set_title(\"Overall Label Distribution\")\n",
        "axes[0].set_xlabel(\"Label\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "# Plot test set label distribution for each fold.\n",
        "for i, (_, test_idx) in enumerate(folds, 1):\n",
        "    test_counts = np.bincount(y_dummy[test_idx], minlength=len(unique_labels))\n",
        "    axes[i].bar(unique_labels, test_counts, tick_label=unique_labels)\n",
        "    axes[i].set_title(f\"Fold {i} Test Distribution\")\n",
        "    axes[i].set_xlabel(\"Label\")\n",
        "    axes[i].set_ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF6EQtuFBcrO"
      },
      "source": [
        "## Part 2: Logistic Regression\n",
        "This part focuses on the implementation of logistic regression (LR). LR is a statistical model used for classification tasks. It predicts the probability of an instance belonging to a particular class by applying the sigmoid function to a linear combination of input features. For binary classification, it outputs probabilities between 0 and 1, assigning the instance to the class with the higher probability.\n",
        "\n",
        "To extend logistic regression for multi-class classification, we use the softmax function, which generalizes the binary logistic regression model to handle multiple classes simultaneously. Softmax-based logistic regression is commonly used in machine learning, particularly for multi-class classification tasks. Softmax function is essentially a maximum likelihood estimation. Recall what we learnt in class, the softmax function can be represented by\n",
        "\n",
        "<img src=\"https://github.com/Michigan-State-University-CSE-440/logistic-regression/blob/main/assets/softmax.png?raw=1\" alt=\"softmax\" width=\"500\"/>\n",
        "\n",
        "Instead of computing a single probability for a binary outcome, softmax outputs a probability distribution over all possible classes. In this approach, we define a separate linear function for each class, computing a raw score (logit, which is ``w*f(x)`` in this side) for each category. The softmax function then converts these logits into 0-1 probabilities by exponentiating each logit and normalizing by the sum of exponentiated logits across all classes. This ensures that the predicted probabilities sum to 1.\n",
        "\n",
        "During training, the model optimizes the deviation between predicted probability with one-hot encoding, which measures how well the predicted probability distribution aligns with the true labels. During testing, the class with the highest probability is chosen as the final prediction.\n",
        "\n",
        "Learning Resources about Softmax function:\n",
        "1. https://www.pinecone.io/learn/softmax-activation/\n",
        "2. https://en.wikipedia.org/wiki/Softmax_function\n",
        "3. https://www.geeksforgeeks.org/the-role-of-softmax-in-neural-networks-detailed-explanation-and-applications/\n",
        "4. https://www.youtube.com/watch?v=8ah-qhvaQqU\n",
        "\n",
        "### Task 3: Logistic Regression Implmentation\n",
        "In this task, you need to implement some key steps of LR. The optimization through gradient descent is fully provided, but you need to try to understand it and **include your understanding of the optimization in you report**.\n",
        "\n",
        "Learning Resources about Gradient descent:\n",
        "1. https://www.youtube.com/watch?v=sDv4f4s2SB8\n",
        "2. https://www.youtube.com/watch?v=qg4PchTECck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LCluT-jBcrO"
      },
      "outputs": [],
      "source": [
        "class MultiClassLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_epochs=1000, batch_size=64, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters:\n",
        "          learning_rate: Step size for gradient descent.\n",
        "          num_epochs: Number of epochs to train.\n",
        "          batch_size: Size of mini-batches.\n",
        "          seed: Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = seed\n",
        "        self.W = None  # Weight matrix will be initialized later\n",
        "\n",
        "    def softmax(self, logits):\n",
        "        \"\"\"\n",
        "        Compute the softmax of the logits.\n",
        "        Args:\n",
        "            logits: Array of shape (n_samples, n_classes).\n",
        "        Returns:\n",
        "            Array of shape (n_samples, n_classes) where each row sums to 1.\n",
        "        \"\"\"\n",
        "\n",
        "        ############TODO:Implement the softmax function following the equation############\n",
        "        ## Hint: Remember to subtract np.max(logits, axis=1, keepdims=True) before exponentiating.\n",
        "        softmax_output =\n",
        "        ##################################################################################\n",
        "\n",
        "        return softmax_output\n",
        "\n",
        "    def one_hot(self, y, num_classes):\n",
        "        \"\"\"\n",
        "        Convert integer labels to one-hot vectors.\n",
        "        Args:\n",
        "            y: Array of shape (n_samples,) with integer labels.\n",
        "            num_classes: Number of classes.\n",
        "        Returns:\n",
        "            Array of shape (n_samples, num_classes) with one-hot encoding.\n",
        "        \"\"\"\n",
        "\n",
        "        ############TODO: Implement one-hot encoding for the label vector y############\n",
        "        ## Hint: Create a zero matrix of shape (number_of_samples, num_classes) and set the appropriate indices to 1.\n",
        "        one_hot_encoded =\n",
        "        ##############################################################################\n",
        "\n",
        "        return one_hot_encoded\n",
        "\n",
        "    def initialize_weights(self, num_features, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize the weight matrix with small random values.\n",
        "        \"\"\"\n",
        "        rng = np.random.default_rng(self.seed)\n",
        "        self.W = rng.normal(loc=0.0, scale=0.01, size=(num_features, num_classes))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the model using mini-batch gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "          X: Input feature matrix of shape (n_samples, n_features).\n",
        "          y: Array of labels of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "        self.initialize_weights(num_features, num_classes)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Shuffle the data using np.random.permutation.\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            # Process mini-batches\n",
        "            for start in range(0, n_samples, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                X_batch = X_shuffled[start:end]   # shape: (batch_size, num_features)\n",
        "                y_batch = y_shuffled[start:end]   # shape: (batch_size,)\n",
        "\n",
        "                ###########TODO: Implement the calculation of probabilities###########\n",
        "                ## step 1: Compute the logits (linear combination of inputs and weights)\n",
        "                ## using dot multiplication between matrices X_batch and self.W\n",
        "                logits =\n",
        "                ## step 2: Call the softmax function to get probabilities\n",
        "                probs =\n",
        "                ## step 3: Call the one-hot function to encode labels\n",
        "                y_one_hot =\n",
        "                ######################################################################\n",
        "\n",
        "                ####Potentially Important: Gradient Calculation and Gradient Descent####\n",
        "                ce_loss = -np.sum(y_one_hot * np.log(probs + 1e-15)) / self.batch_size\n",
        "                grad = np.dot(X_batch.T, probs-y_one_hot) / self.batch_size\n",
        "                self.W -= self.learning_rate * grad\n",
        "                ######################################################################\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for input data.\n",
        "        Args:\n",
        "            X: Input feature matrix of shape (n_samples, n_features).\n",
        "        Returns:\n",
        "            Array of predicted labels of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        #########################TODO: label prediction#########################\n",
        "        ## step 1: Compute the logits similar to the fit function\n",
        "        logits =\n",
        "        ## step 2: Apply the softmax function to get probabilities\n",
        "        probs =\n",
        "        ## step 3: Select the index with the highest probability for each sample as the label prediction\n",
        "        pred =\n",
        "        ##########################################################################\n",
        "\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izIDcszGBcrP"
      },
      "source": [
        "## Part 3: Multi-class Classification (No Task)\n",
        "Finally, we can achieve the multi-class classification by putting everthing we have done together, including the data curation and modeling, to form a complete pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCpw-M2GBcrP"
      },
      "outputs": [],
      "source": [
        "root_dir = \"Celebrity_Faces_Dataset\"\n",
        "seed=440\n",
        "# below are some hyperparameters, you can feel free to change them\n",
        "# and see how they affect the model performance\n",
        "k=5\n",
        "pca_components=100\n",
        "learning_rate=0.01\n",
        "num_epochs=200\n",
        "batch_size=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDMH1mzJBcrP"
      },
      "outputs": [],
      "source": [
        "# 1. Load data\n",
        "X_gray, y, label_map, X_color = load_celebrity_faces(root_dir, image_size=(64, 64))\n",
        "print(f\"Loaded {len(X_gray)} images for {len(label_map)} celebrities.\\n\")\n",
        "\n",
        "# 2. Normalize grayscale features to [0..1] for training\n",
        "X_gray = X_gray / 255.0\n",
        "\n",
        "# 3. Perform manual k-fold splitting\n",
        "folds = kfold_split(X_gray, y, k=k)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "## these three parameters are used to store the data for visualizing the classirication results,\n",
        "## no need to pay much attention to them\n",
        "last_y_test = None\n",
        "last_y_pred = None\n",
        "last_test_indices = None\n",
        "\n",
        "fold_num = 1\n",
        "for train_indices, test_indices in folds:\n",
        "    print(f\"=== Fold {fold_num}/{k} ===\")\n",
        "    fold_num += 1\n",
        "\n",
        "    ## get the training and testing data based on the indices\n",
        "    X_train = X_gray[train_indices]\n",
        "    y_train = y[train_indices]\n",
        "    X_test = X_gray[test_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    #########Potentially Important: Apply PCA to the image#########\n",
        "    ## PCA is used for Dimensionality Reduction and Feature Extraction, which\n",
        "    ## can help to reduce the computational cost and improve the model performance\n",
        "    pca = PCA(n_components=pca_components, random_state=seed)\n",
        "    pca.fit(X_train)\n",
        "    X_train = pca.transform(X_train)\n",
        "    X_test = pca.transform(X_test)\n",
        "    ############################################################\n",
        "\n",
        "    # 4. initalize and train logistic regression model\n",
        "    model = MultiClassLogisticRegression(\n",
        "        learning_rate,\n",
        "        num_epochs,\n",
        "        batch_size,\n",
        "        seed\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Evaluate on the testing data\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = np.mean(y_pred == y_test)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Fold accuracy: {accuracy*100:.2f}%\\n\")\n",
        "\n",
        "    # keep data for final fold visualization\n",
        "    last_y_test = y_test\n",
        "    last_y_pred = y_pred\n",
        "    last_test_indices = test_indices\n",
        "\n",
        "# 6. Print results across folds\n",
        "mean_acc = np.mean(accuracies)\n",
        "std_acc = np.std(accuracies)\n",
        "print(f\"=== {k}-Fold Final Results ===\")\n",
        "print(\"Accuracies per fold:\", [f\"{acc*100:.2f}%\" for acc in accuracies])\n",
        "print(f\"Mean accuracy: {mean_acc*100:.2f}%\")\n",
        "print(f\"Std deviation: {std_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojjnFLieBcrP"
      },
      "source": [
        "## Part 4: Visualization (No Task)\n",
        "In this part, we visualize some classification reults on the testing data. Don't be suprised with mistakes the model makes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHJ6fpP1BcrP"
      },
      "outputs": [],
      "source": [
        "if last_y_test is not None and last_y_pred is not None:\n",
        "\n",
        "    # (C) Show exactly 1 correct and 1 incorrect prediction for each label (if available)\n",
        "    unique_labels = np.unique(last_y_test)\n",
        "    for lbl in unique_labels:\n",
        "        # indices for this label\n",
        "        lbl_indices = np.where(last_y_test == lbl)[0]\n",
        "\n",
        "        # among these, figure out which are correct and incorrect\n",
        "        correct_mask = (last_y_pred[lbl_indices] == lbl)\n",
        "        incorrect_mask = ~correct_mask\n",
        "\n",
        "        correct_indices = lbl_indices[np.where(correct_mask)[0]]\n",
        "        incorrect_indices = lbl_indices[np.where(incorrect_mask)[0]]\n",
        "\n",
        "        # Show 1 correct example if available\n",
        "        if len(correct_indices) > 0:\n",
        "            idx = correct_indices[0]\n",
        "            img_rgb = X_color[last_test_indices[idx]]\n",
        "            pred_label = last_y_pred[idx]\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(img_rgb)\n",
        "            plt.title(f\"True: {label_map[lbl]} | Predicted: {label_map[pred_label]} (Correct)\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "        # Show 1 incorrect example if available\n",
        "        if len(incorrect_indices) > 0:\n",
        "            idx = incorrect_indices[0]\n",
        "            img_rgb = X_color[last_test_indices[idx]]\n",
        "            pred_label = last_y_pred[idx]\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(img_rgb)\n",
        "            plt.title(f\"True: {label_map[lbl]} | Predicted: {label_map[pred_label]} (Incorrect)\")\n",
        "            plt.axis(\"off\")\n",
        "        plt.subplots_adjust(wspace=1.5)\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}