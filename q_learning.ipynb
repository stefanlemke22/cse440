{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5kwX7hOAcfp"
      },
      "source": [
        "# Lab 3: Q-learning\n",
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAsNRK1nAcfr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# You will need numpy and gym. You can try running the following lines to install them\n",
        "# The assignment is tested on Python3.8 so in case you are having installation issues you might\n",
        "# want to try installing that version.\n",
        "\n",
        "!{os.sys.executable} -m pip install numpy\n",
        "!{os.sys.executable} -m pip install gymnasium\n",
        "!{os.sys.executable} -m pip install Pillow\n",
        "!{os.sys.executable} -m pip install ipython\n",
        "!{os.sys.executable} -m pip install pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cak4N1YAcfs"
      },
      "outputs": [],
      "source": [
        "# Import the environment we will use in this assignment\n",
        "env=gym.make('FrozenLake-v1',is_slippery=False, render_mode=\"rgb_array\").unwrapped\n",
        "\n",
        "# Show the model\n",
        "print(f\"Number of States {env.observation_space.n}, Number of Actions {env.action_space.n}\")\n",
        "print(f\"Reward range {env.reward_range}\")\n",
        "# reset the environment\n",
        "env.reset()\n",
        "# visualize the current state\n",
        "display(Image.fromarray(env.render()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ijy44FAcft"
      },
      "source": [
        "## Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ2vYJbVAcfu"
      },
      "outputs": [],
      "source": [
        "# Training parameters common to both methods.\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "num_episodes = 2000\n",
        "max_steps = 100\n",
        "alpha = 0.8        # Learning rate\n",
        "gamma = 0.95       # Discount factor\n",
        "epsilon_init = 1.0 # Initial exploration rate\n",
        "epsilon_min = 0.01 # Minimum exploration rate\n",
        "epsilon_decay = 0.995"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzPbfQgzAcfu"
      },
      "source": [
        "## Part 1: Q-Table Based Q-Learning\n",
        "### Task 1: Complete the table-based Q-learning algorithm\n",
        "**Coding Instructions:**\n",
        "1. First, you need to complete the epsilon-greedy action selection. In each eposide we have an exploration rate *epsilon*. Compare a random number with epsilon: If the random number is less than epsilon, explore by choosing a random action. Otherwise, exploit by choosing the action with the highest Q-value in the current state.\n",
        "   \n",
        "2. Second, you need to do Q-learning update. You may consider update the Q-value via the following equation:  \n",
        "$$\n",
        "Q(\\text{state}, action) \\leftarrow Q(\\text{state}, action)\n",
        "\\;+\\; \\alpha\n",
        "\\Bigl[\n",
        "  R_t + \\gamma \\max_a Q(\\text{next\\_state}, a)\n",
        "  \\;-\\;\n",
        "  Q(\\text{state}, action)\n",
        "\\Bigr]\n",
        "$$\n",
        "This incrementally updates the Q-value estimate toward the immediate reward + discounted best future Q-value. Over many episodes, the Q-table converges to accurate estimates of how good each (state, action) pair is, assuming sufficient exploration and proper hyperparameters.\n",
        "\n",
        "3. Third, after each episode, you typically decrease (or “decay”) epsilon so the agent explores less over time. A common formular is\n",
        "$$\n",
        "\\epsilon \\leftarrow \\max\\bigl(\\epsilon \\times \\epsilon_{\\text{decay}},\\; \\epsilon_{\\min}\\bigr)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPRV5GbbAcfv"
      },
      "outputs": [],
      "source": [
        "def train_q_table(num_episodes, max_steps, alpha, gamma,\n",
        "                  epsilon_init, epsilon_min, epsilon_decay):\n",
        "    \"\"\"\n",
        "    Trains a Q-learning agent in the FrozenLake-v1 environment.\n",
        "    \"\"\"\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    Q = np.zeros((state_size, action_size))\n",
        "    epsilon = epsilon_init\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            ##########TODO: Implement the epsilon-greedy action selection##########\n",
        "            action = # Your code here, don't have to finish in on line\n",
        "            #######################################################################\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            ##########TODO: Implement the Q-learning update ##################\n",
        "            Q[state, action] = # Your code here\n",
        "            ################################################################\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        ############TODO: Implement the epsilon decay####################\n",
        "        epsilon = # Your code here\n",
        "        ##################################################################\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        # Optional logging every 500 episodes\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Q-table - Episode {episode+1}/{num_episodes} \"\n",
        "                  f\"- Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    return Q, rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P61_PVvCAcfw"
      },
      "source": [
        "### Call the Q-learning function and visualize the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIuyNXE_Acfx"
      },
      "outputs": [],
      "source": [
        "print(\"Training Q-table agent...\")\n",
        "Q_table, rewards_q_table = train_q_table(num_episodes, max_steps, alpha, gamma,\n",
        "                                           epsilon_init, epsilon_min, epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzp7sxROAcfy"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Policy Visualization Functions\n",
        "# -------------------------------\n",
        "def visualize_policy_from_q(Q):\n",
        "    \"\"\"\n",
        "    Visualizes the learned policy from a Q-table.\n",
        "    Frozen cells ('F') are replaced by arrows indicating the best action.\n",
        "    \"\"\"\n",
        "    action_symbols = {0: '<', 1: 'v', 2: '>', 3: '^'}\n",
        "    grid = env.desc.astype('U')\n",
        "    policy_grid = np.copy(grid)\n",
        "    n_rows, n_cols = grid.shape\n",
        "    for row in range(n_rows):\n",
        "        for col in range(n_cols):\n",
        "            state_index = row * n_cols + col\n",
        "            if grid[row, col] == 'F':\n",
        "                best_action = np.argmax(Q[state_index])\n",
        "                policy_grid[row, col] = action_symbols[best_action]\n",
        "    for row in policy_grid:\n",
        "        print(\" \".join(row))\n",
        "\n",
        "print(\"\\nLearned Policy from Q-table Agent:\")\n",
        "visualize_policy_from_q(Q_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOv6CZWFAcfz"
      },
      "source": [
        "## Part 2: Linear Q-Function Approximator\n",
        "### Task 2: Read through the code\n",
        "In this part there is no coding tasks, but you need to carefully read through and understand the implementation of Q-Function Approximator, which should be also **included in your report**.\n",
        "\n",
        "External learning resources:\n",
        "- https://gibberblot.github.io/rl-notes/single-agent/function-approximation.html\n",
        "- https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-q-learning-and-linear-function-approximation/\n",
        "- https://www.youtube.com/watch?v=wAk1lxmiW4c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqBXj4O6Acf0"
      },
      "outputs": [],
      "source": [
        "class LinearQAgent:\n",
        "    def __init__(self, state_size, action_size, alpha, gamma, epsilon, epsilon_min, epsilon_decay):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.weights = np.zeros((state_size, action_size))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_size)  # random action\n",
        "        # Exploit: linear model simply picks argmax among w[state]\n",
        "        return np.argmax(self.weights[state])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Perform a Q-learning update with linear approximation.\"\"\"\n",
        "        q_value = self.weights[state, action]\n",
        "        next_q = np.max(self.weights[next_state]) if not done else 0.0\n",
        "        # Q-learning update rule\n",
        "        difference = reward + self.gamma * next_q - q_value\n",
        "        self.weights[state, action] += self.alpha * difference\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate.\"\"\"\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "    def predict(self, state):\n",
        "        \"\"\"\n",
        "        Returns a NumPy array of Q-values for all actions at the given state.\n",
        "        Useful for visualizing or evaluating the learned policy.\n",
        "        \"\"\"\n",
        "        return self.weights[state]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9zCGV1LAcf0"
      },
      "outputs": [],
      "source": [
        "def train_linear_agent(\n",
        "    num_episodes,\n",
        "    max_steps,\n",
        "    alpha,\n",
        "    gamma,\n",
        "    epsilon,\n",
        "    epsilon_min,\n",
        "    epsilon_decay\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a Q-learning agent using a linear function approximator in a Gym/Gymnasium environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Derive state_size and action_size\n",
        "    # If the environment is discrete (like FrozenLake), we can do:\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Initialize the Linear Q agent\n",
        "    agent = LinearQAgent(state_size, action_size, alpha, gamma, epsilon, epsilon_min, epsilon_decay)\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # For Gymnasium or Gym >= 0.26, reset() returns (obs, info)\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update the agent\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Decay epsilon at the end of each episode\n",
        "        agent.decay_epsilon()\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            print(f\"Linear Q - Episode {episode+1}/{num_episodes} - Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMfwT5G1Acf0"
      },
      "source": [
        "### Call the Q-learning function and visualize the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtaUdWrmAcf1"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTraining Linear Q-function agent...\")\n",
        "linear_agent, rewards_linear = train_linear_agent(num_episodes, max_steps, alpha, gamma,\n",
        "                                                    epsilon_init, epsilon_min, epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYxcGHdRAcf1"
      },
      "outputs": [],
      "source": [
        "def visualize_policy_from_linear(agent):\n",
        "    \"\"\"\n",
        "    Visualizes the learned policy from the linear Q-function approximator.\n",
        "    In FrozenLake, frozen cells ('F') are replaced by arrows indicating the best action.\n",
        "    \"\"\"\n",
        "    action_symbols = {0: '<', 1: 'v', 2: '>', 3: '^'}\n",
        "\n",
        "    # Access the underlying desc (ASCII map) as a character array.\n",
        "    # For Gymnasium/Gym v0.26+, often it is env.unwrapped.desc\n",
        "    grid = env.unwrapped.desc.astype('U')  # 'U' => Unicode string type\n",
        "    policy_grid = np.copy(grid)\n",
        "    n_rows, n_cols = grid.shape\n",
        "\n",
        "    for row in range(n_rows):\n",
        "        for col in range(n_cols):\n",
        "            # Convert (row, col) to state index in FrozenLake\n",
        "            state_index = row * n_cols + col\n",
        "\n",
        "            # Only overwrite if the cell is 'F' (frozen)\n",
        "            if grid[row, col] == 'F':\n",
        "                # agent.predict(state_index) should return a Q-value array [Q(s,a0), Q(s,a1), ...]\n",
        "                q_values = agent.predict(state_index)\n",
        "                best_action = np.argmax(q_values)\n",
        "                policy_grid[row, col] = action_symbols[best_action]\n",
        "\n",
        "    # Print the resulting policy map\n",
        "    for row_content in policy_grid:\n",
        "        print(\" \".join(row_content))\n",
        "\n",
        "print(\"\\nLearned Policy from Linear Q-function Agent:\")\n",
        "visualize_policy_from_linear(linear_agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWblOZIeAcf2"
      },
      "source": [
        "## Part 3: Comparsion of Q-Table with Q-function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhWDt7qBAcf3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_episodes = 5000\n",
        "max_steps = 100\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon_init = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "# Train the tabular Q agent\n",
        "q_table, rewards_tabular = train_q_table(num_episodes, max_steps,\n",
        "                                         alpha, gamma,\n",
        "                                         epsilon_init, epsilon_min,\n",
        "                                         epsilon_decay)\n",
        "\n",
        "# Train the linear Q agent\n",
        "linear_agent, rewards_linear = train_linear_agent(\n",
        "    num_episodes=num_episodes,\n",
        "    max_steps=max_steps,\n",
        "    alpha=alpha,\n",
        "    gamma=gamma,\n",
        "    epsilon=epsilon_init,\n",
        "    epsilon_min=epsilon_min,\n",
        "    epsilon_decay=epsilon_decay\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAyxLSJ8Acf3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an array of episodes for the x-axis\n",
        "episodes = np.arange(num_episodes)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "window_size = 100\n",
        "rolling_tabular = np.convolve(rewards_tabular, np.ones(window_size)/window_size, mode='valid')\n",
        "rolling_linear = np.convolve(rewards_linear, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "# Adjust the x-axis for the rolling averages\n",
        "rolling_episodes = np.arange(window_size - 1, num_episodes)\n",
        "\n",
        "plt.plot(rolling_episodes, rolling_tabular, label=f\"Tabular (MA-{window_size})\", linewidth=2.0)\n",
        "plt.plot(rolling_episodes, rolling_linear, label=f\"Linear (MA-{window_size})\", linewidth=2.0)\n",
        "\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Performance Comparison: Tabular vs. Linear Q-learning\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGKriEgSAcf4"
      },
      "source": [
        "## (Bonus) Part 4: Deep Q-Learning\n",
        "This is a bonus part including two tasks: Task 3 and Task 4, which is not compulsory to finish. If you complete this part and get correct results, one extra point will be added to your grade.\n",
        "\n",
        "External learning resources:\n",
        "- https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm\n",
        "- https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
        "- https://www.youtube.com/watch?v=wDVteayWWvU\n",
        "### Task 3: Complete the Deep Q-learning network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV7iS4QeAcf4"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=32):\n",
        "        super(DQNetwork, self).__init__()\n",
        "\n",
        "        #######TODO: Implement the network architecture########\n",
        "        # here is a simple network with one hidden layer, you can try different architectures\n",
        "        # and parameters to improve the performance\n",
        "        # self.net = nn.Sequential(\n",
        "        #     nn.Linear(state_size, hidden_size),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(hidden_size, action_size)\n",
        "        # )\n",
        "        #######################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k09jL-RXAcf4"
      },
      "source": [
        "### Defionition of helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BFyXd0uAcf5"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV9viDJfAcf5"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(state_idx, state_size):\n",
        "    vec = np.zeros(state_size, dtype=np.float32)\n",
        "    vec[state_idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "def select_action(network, state, epsilon, action_size):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy policy.\n",
        "    'state' is a 1D PyTorch tensor (already one-hot encoded).\n",
        "    \"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(action_size)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values = network(state)\n",
        "            return torch.argmax(q_values).item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUtTckZuAcf5"
      },
      "source": [
        "### Task 4: Complete the training steps of Deep Q-learning\n",
        "In this task you need to complete the training function of Deep Q-learning. Most of the code are ready for you, only some key parts are masked out.\n",
        "\n",
        "**Coding Instructions:**\n",
        "1. Forward the Q-network for current states: You already have a batch of current states in states_t (shape [batch_size, state_dim]). Simply pass states_t through your Q-network to get the Q-values for each action.\n",
        "2. Forward the Q-network for next states and compute max *Q*: Pass the next states in next_states_t through the Q-network. Then take the max across actions to get the best possible future Q-value for each next state. We usually do this under a torch.no_grad() context or detach the graph, because we do not want to backpropagate through the next state’s Q-values in standard DQN.\n",
        "3. Decay the epsilon as in previous tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUYTdkHXAcf5"
      },
      "outputs": [],
      "source": [
        "def train_dqn_frozenlake(\n",
        "    env_name=\"FrozenLake-v1\",\n",
        "    num_episodes=2000,\n",
        "    max_steps=100,\n",
        "    gamma=0.99,\n",
        "    lr=1e-3,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=0.999,\n",
        "    batch_size=32,\n",
        "    replay_capacity=10000,\n",
        "    hidden_size=32\n",
        "):\n",
        "    # 1. Create environment\n",
        "    env = gym.make(env_name, is_slippery=True)\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # 2. Initialize network & optimizer\n",
        "    q_network = DQNetwork(state_size, action_size, hidden_size)\n",
        "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # 3. Initialize replay buffer\n",
        "    replay_buffer = ReplayBuffer(capacity=replay_capacity)\n",
        "\n",
        "    # 4. Epsilon initialization\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    # 5. For logging\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Gymnasium: reset() returns (obs, info)\n",
        "        state_idx, _ = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Convert state to one-hot + torch tensor\n",
        "            state_oh = torch.tensor(one_hot_encode(state_idx, state_size), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            # Select action\n",
        "            action = select_action(q_network, state_oh, epsilon, action_size)\n",
        "\n",
        "            # Step in the environment\n",
        "            next_state_idx, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Store transition in buffer\n",
        "            next_state_oh = one_hot_encode(next_state_idx, state_size)\n",
        "            replay_buffer.push(state_oh.squeeze(0).numpy(), action, reward, next_state_oh, done)\n",
        "\n",
        "            state_idx = next_state_idx\n",
        "            total_reward += reward\n",
        "\n",
        "            # Train the network if replay buffer has enough samples\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                # Sample a mini-batch\n",
        "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "                # Convert all to tensors\n",
        "                states_t = torch.tensor(states, dtype=torch.float32)      # (batch_size, state_size)\n",
        "                actions_t = torch.tensor(actions, dtype=torch.long)       # (batch_size,)\n",
        "                rewards_t = torch.tensor(rewards, dtype=torch.float32)    # (batch_size,)\n",
        "                next_states_t = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones_t = torch.tensor(dones, dtype=torch.float32)        # (batch_size,)\n",
        "\n",
        "                #########TODO: forward the Q-network and compute the loss########\n",
        "                q_values = # Your code here\n",
        "                #################################################################\n",
        "\n",
        "                # Gather the Q-value for the chosen action: shape (batch_size,)\n",
        "                q_values_current = q_values.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    #########TODO: forward Q-network again for next states and compute the max Q-value########\n",
        "                    q_next_max = # Your code here\n",
        "                    ##########################################################################################\n",
        "\n",
        "                # Target: y = r + gamma * max Q(next_state) if not done\n",
        "                q_targets = rewards_t + gamma * q_next_max * (1 - dones_t)\n",
        "\n",
        "                loss = loss_fn(q_values_current, q_targets)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        ######################TODO: Decay epsilon#####################\n",
        "        epsilon = # Your code here\n",
        "        ##############################################################\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "\n",
        "        # Optional logging\n",
        "        if (episode + 1) % 200 == 0:\n",
        "            print(f\"Episode {episode+1}/{num_episodes}, Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return q_network, rewards_per_episode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZwGOtfWAcf5"
      },
      "source": [
        "### Call the Deep Q-learning function and visualize the reward across training episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDlh4SQBAcf6"
      },
      "outputs": [],
      "source": [
        "q_network, rewards = train_dqn_frozenlake(num_episodes=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfgZ_ZjIAcf6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "window_size = 100\n",
        "rolling_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "plt.plot(rolling_rewards)\n",
        "plt.title(\"DQN Training: Rolling Rewards\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(f\"Reward (window_size={window_size})\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}